{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af7bec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andrewmartinson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andrewmartinson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/andrewmartinson/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrewmartinson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import altair as alt\n",
    "\n",
    "import re \n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pickle\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from nltk.corpus import stopwords #stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import spacy #used for lemmatization nltk.download('stopwords')\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#import pyLDAvis.gensim_models\n",
    "from gensim.models.coherencemodel import CoherenceModel \n",
    "#stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Topic model\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "\n",
    "# Text preprocessiong\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d435cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the CFPB Complaints Database\n",
    "\n",
    "df=pd.read_csv('complaints.csv', low_memory=False)\n",
    "df['Date_received_dt'] = pd.to_datetime(df['Date received'], format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2967cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a smaller dataframe based on dates being used for analysis\n",
    "df_date = df[(df['Date received'] >= '2015-01-01') & (df['Date received'] <= '2016-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f334aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Identify the 5 banks or financial services companies with the most complaints\n",
    "#Don't run if comparing the same banks over a period of time, only run for the first time period\n",
    "top5=df_date.groupby('Company')['Complaint ID'].count().reset_index(name='count') \\\n",
    "            .sort_values(['count'], ascending=False) \\\n",
    "            .head(5)\n",
    "\n",
    "\n",
    "#create list of top 5 companies for use in analysis\n",
    "top_5_list = list(top5['Company'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a820a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing text = text.lower()'''\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[x*\\/]', '', text) #remove those dates with the format of XX/XX OR XXXX/XX/XX\n",
    "    text = re.sub('x', '', text)#remove \\n\n",
    "    text = re.sub('\\n', '', text)#remove \\n\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text) #remove bracketed text that appears like a list\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text(x)\n",
    "\n",
    "\n",
    "\n",
    "#Cycle through the bank data for comparison purposes\n",
    "def extract_topics(list_of_companies, df_date):\n",
    "\n",
    "    for company in tqdm(list_of_companies):\n",
    "        df_bank = None\n",
    "        data_clean = None\n",
    "        topic_model = None\n",
    "        topics = None\n",
    "        probabilities = None\n",
    "        topic_model = None\n",
    "\n",
    "\n",
    "        df_bank = df_date[df_date['Company'] == company]\n",
    "\n",
    "        #get rid of rows with null values in the customer narrative\n",
    "        df_bank = df_bank.dropna(subset=['Consumer complaint narrative'])\n",
    "\n",
    "        # Text cleaning - lowercase, remove punctuations and remove words with numbers.\n",
    "        data_clean = pd.DataFrame(df_bank['Consumer complaint narrative'].apply(round1)) \n",
    "\n",
    "        # Remove stopwords\n",
    "        data_clean['narrative_without_stopwords'] = data_clean['Consumer complaint narrative'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))# Lemmatization\n",
    "        data_clean['narrative_lemmatized'] = data_clean['narrative_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))# Take a look at the data\n",
    "\n",
    "        #Must reset index to otherwise BERTopic will throw errors\n",
    "        data_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Initiate BERTopic\n",
    "        # topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True)\n",
    "        topic_model = BERTopic(language=\"english\", calculate_probabilities=True).fit(data_clean['narrative_without_stopwords'])\n",
    "\n",
    "        #get the topics from the model\n",
    "        tm_df = topic_model.get_topic_info()\n",
    "\n",
    "        #save the topics into a csv file\n",
    "        filenme = company+'_topics_2018-19'\n",
    "        savepath = filenme+'.csv'\n",
    "        tm_df.to_csv(path_or_buf=savepath)\n",
    "\n",
    "        #save the model for later use\n",
    "        topic_model.save(company+'_model')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1027ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewmartinson/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      " 20%|████████▏                                | 1/5 [21:58<1:27:54, 1318.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewmartinson/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      " 40%|████████████████▍                        | 2/5 [40:38<1:00:05, 1201.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewmartinson/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      " 60%|█████████████████████████▊                 | 3/5 [59:36<39:05, 1172.67s/it]"
     ]
    }
   ],
   "source": [
    "#create a smaller dataframe based on dates being used for analysis\n",
    "df_date = None\n",
    "df_date = df[(df['Date received'] >= '2018-01-01') & (df['Date received'] <= '2019-12-31')]\n",
    "extract_topics(top_5_list, df_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "798ba2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate UMAP\n",
    "umap_model = None\n",
    "umap_model = UMAP(n_neighbors=15, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0,\n",
    "                  metric='cosine', \n",
    "                  random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d3e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
